# 前向神经网络

## 写出常用激活函数及其导数。

## 神经网络训练时是否可以将参数全部初始化为0？

## 多层感知机表示异或逻辑时最少需要几个隐藏层？

## 为什么Sigmoid和Tanh激活函数会使梯度消失？

## 写出多层感知机的平方误差和交叉熵损失函数。

## 解释卷积操作中的稀疏交互和参数共享及其作用。

## 一个隐藏层需要多少隐节点能够实现包含n元输入的任意布尔函数？

## 多个隐层实现包含n元输入的任意布尔函数最少需要多少个节点和网络层？

## ReLU系列的激活函数的优点是什么？他们有什么局限性以及如何改进？

## 平方误差损失函数和交叉损失函数分别适合什么场景？

## 为什么Dropout可以抑制过拟合？简述它的工作原理和实现。

## 批量归一化的基本动机与原理是什么？在卷积神经网络中如何使用？

## 常见的池化操作有哪些？池化的作用是什么？

## 卷积神经网络如何用于文本分类任务？

## ResNet的提出背景和核心理论是什么？

## 根据损失函数推导各层参数更新的梯度计算公式。
